## 1. ConcurrentHashMap
### 1. JDK1.7版本
采用**分段锁**（锁的是不同的`HashEntry`数组，即一段数据），将HashMap中的`table`改变为使用`segment`数组，`segment`类中使用`ReentrantLock`实现锁。每个`segment`元素中包含一个`HashEntry`数组，HashEntry数组与HashMap结构一致，均为数组+链表结构

<img src="D:\Project\IT-notes\Java\并发\img\JDK1.7的ConcurrentHashMap结构.png" style="width:700px;height:300px;" />

<img src="D:\Project\IT-notes\Java\并发\img\JDK1.7的ConcurrentHashMap包继承关系.png" style="width:400px;height:500px;" />

#### 部分源码
```java
/**Segment继承了ReentrantLock，所以它就是一种可重入锁（ReentrantLock)。
 * 在ConcurrentHashMap，一个Segment就是一个子哈希表
 * Segment里维护了一个HashEntry数组
 * 并发环境下，对于不同Segment的数据进行操作是不用考虑锁竞争的
 */
final Segment<K,V>[] segments;

/**
 * Segment类似于HashMap，一个Segment维护着一个HashEntry数组
 * HashEntry是目前我们提到的最小的逻辑处理单元了
 * 一个ConcurrentHashMap维护一个Segment数组
 * 一个Segment维护一个HashEntry数组
 */
transient volatile HashEntry<K,V>[] table;
```

```java
//HashEntry初始化
static final class HashEntry<K,V> {
	final int hash;
	final K key;
	volatile V value;
	volatile HashEntry<K,V> next;
	//其他省略
}
```

```java
//Segment初始化
Segment(float lf, int threshold, HashEntry<K,V>[] tab) {
	//负载因子
	this.loadFactor = lf;
	//阈值
	this.threshold = threshold;
	//主干数组即HashEntry数组
	this.table = tab;
}
```

```java
//ConcurrentHashMap构造方法
public ConcurrentHashMap(int initialCapacity, float loadFactor, int concurrencyLevel) {
	  if (!(loadFactor > 0) || initialCapacity < 0 || concurrencyLevel <= 0)
		  throw new IllegalArgumentException();
	  //MAX_SEGMENTS 为1<<16=65536，也就是最大并发数为65536
	  if (concurrencyLevel > MAX_SEGMENTS)
		  concurrencyLevel = MAX_SEGMENTS;
	  //2的sshif次方等于ssize，例:ssize=16,sshift=4;ssize=32,sshif=5
	 int sshift = 0;
	 //ssize 为segments数组长度，根据concurrentLevel计算得出
	 int ssize = 1;
	 while (ssize < concurrencyLevel) {
		 ++sshift;
		 ssize <<= 1;
	 }
	 //segmentShift和segmentMask这两个变量在定位segment时会用到，后面会详细讲
	 this.segmentShift = 32 - sshift;
	 this.segmentMask = ssize - 1;
	 if (initialCapacity > MAXIMUM_CAPACITY)
		 initialCapacity = MAXIMUM_CAPACITY;
	 //计算cap的大小，即Segment中HashEntry的数组长度，cap也一定为2的n次方.
	 int c = initialCapacity / ssize;
	 if (c * ssize < initialCapacity)
		 ++c;
	 int cap = MIN_SEGMENT_TABLE_CAPACITY;
	 while (cap < c)
		 cap <<= 1;
	 //创建segments数组并初始化第一个Segment，其余的Segment延迟初始化
	 Segment<K,V> s0 =
		 new Segment<K,V>(loadFactor, (int)(cap * loadFactor),
						  (HashEntry<K,V>[])new HashEntry[cap]);
	 Segment<K,V>[] ss = (Segment<K,V>[])new Segment[ssize];
	 UNSAFE.putOrderedObject(ss, SBASE, s0);
	 this.segments = ss;
 }
```

**segmentMask**：段掩码，假如segments数组长度为16，则段掩码为16-1=15；segments长度为32，段掩码为32-1=31。这样得到的所有bit位都为1，可以更好地保证散列的均匀性

**segmentShift**：2的sshift次方等于ssize，segmentShift=32-sshift。若segments长度为16，segmentShift=32-4=28;若segments长度为32，segmentShift=32-5=27。而计算得出的hash值最大为32位，无符号右移segmentShift，则意味着只保留高几位（其余位是没用的），然后与段掩码segmentMask位运算来定位Segment

```java
public V put(K key, V value) {
	Segment<K,V> s;
	//concurrentHashMap不允许key/value为空
	if (value == null)
		throw new NullPointerException();
	//hash函数对key的hashCode重新散列，避免差劲的不合理的hashcode，保证散列均匀
	int hash = hash(key);
	//返回的hash值无符号右移segmentShift位与段掩码进行位运算，定位segment
	int j = (hash >>> segmentShift) & segmentMask;
	if ((s = (Segment<K,V>)UNSAFE.getObject          // nonvolatile; recheck
		 (segments, (j << SSHIFT) + SBASE)) == null) //  in ensureSegment
		s = ensureSegment(j);
	return s.put(key, hash, value, false);
}
```

```java
//get/put方法

public V get(Object key) {
	Segment<K,V> s; 
	HashEntry<K,V>[] tab;
	int h = hash(key);
	long u = (((h >>> segmentShift) & segmentMask) << SSHIFT) + SBASE;        //先定位Segment，再定位HashEntry
	if ((s = (Segment<K,V>)UNSAFE.getObjectVolatile(segments, u)) != null &&
		(tab = s.table) != null) {
		for (HashEntry<K,V> e = (HashEntry<K,V>) UNSAFE.getObjectVolatile
				 (tab, ((long)(((tab.length - 1) & h)) << TSHIFT) + TBASE);
			 e != null; e = e.next) {
			K k;
			if ((k = e.key) == key || (e.hash == h && key.equals(k)))
				return e.value;
		}
	}
	return null;
}

final V put(K key, int hash, V value, boolean onlyIfAbsent) {
	HashEntry<K,V> node = tryLock() ? null :
		scanAndLockForPut(key, hash, value);
		//tryLock不成功时会遍历定位到的HashEnry位置的链表（遍历主要是为了使CPU缓存链表）
		//若找不到，则创建HashEntry
		//tryLock一定次数后（MAX_SCAN_RETRIES变量决定）则lock
		//若遍历过程中，由于其他线程的操作导致链表头结点变化，则需要重新遍历
	V oldValue;
	try {
		HashEntry<K,V>[] tab = table;
		int index = (tab.length - 1) & hash;//定位HashEntry，可以看到，这个hash值在定位Segment时和在Segment中定位HashEntry都会用到，只不过定位Segment时只用到高几位。
		HashEntry<K,V> first = entryAt(tab, index);
		for (HashEntry<K,V> e = first;;) {
			if (e != null) {
				K k;
				if ((k = e.key) == key ||
					(e.hash == hash && key.equals(k))) {
					oldValue = e.value;
					if (!onlyIfAbsent) {
						e.value = value;
						++modCount;
					}
					break;
				}
				e = e.next;
			}
			else {
				if (node != null)
					node.setNext(first);
				else
					node = new HashEntry<K,V>(hash, key, value, first);
				int c = count + 1;
				//若c超出阈值threshold，需要扩容并rehash
				//扩容后的容量是当前容量的2倍
				//这样可以最大程度避免之前散列好的entry重新散列
				if (c > threshold && tab.length < MAXIMUM_CAPACITY)
					rehash(node);
				else
					setEntryAt(tab, index, node);
				++modCount;
				count = c;
				oldValue = null;
				break;
			}
		}
	} finally {
		unlock();
	}
	return oldValue;
}
```

### 2. JDK1.8版本
1.8的实现已经抛弃了`Segment`分段锁机制，利用`CAS+Synchronized`来保证并发更新的安全，底层采用数组+链表+红黑树的存储结构

**JDK7与JDK8中HashMap的大致变化**：
1. 1.7中采用数组+链表，1.8采用的是数组+链表/红黑树，即在1.7中链表长度超过一定长度后就改成红黑树存储
2. 1.7扩容时需要重新计算哈希值和索引位置，1.8并不重新计算哈希值，巧妙地采用和扩容后容量进行&操作来计算新的索引位置
3. 1.7是采用表头插入法插入链表，1.8采用的是尾部插入法
4. 在1.7中采用表头插入法，在扩容时会改变链表中元素原本的顺序，以至于在并发场景下导致链表成环的问题；在1.8中采用尾部插入法，在扩容时会保持链表元素原本的顺序，就不会出现链表成环的问题了

<img src="D:\Project\IT-notes\Java\并发\img\JDK1.8的ConcurrentHashMap结构.png" style="width:700px;height:200px;" />

#### 部分源码
```java
public V get(Object key) {
    Node<K,V>[] tab; Node<K,V> e, p; int n, eh; K ek;
    int h = spread(key.hashCode()); //计算两次hash
    if ((tab = table) != null && (n = tab.length) > 0 &&
        (e = tabAt(tab, (n - 1) & h)) != null) {//读取首节点的Node元素
        if ((eh = e.hash) == h) { //如果该节点就是首节点就返回
            if ((ek = e.key) == key || (ek != null && key.equals(ek)))
                return e.val;
        }
        //hash值为负值表示正在扩容，这个时候查的是ForwardingNode的find方法来定位到nextTable来
        //查找，查找到就返回
        else if (eh < 0)
            return (p = e.find(h, key)) != null ? p.val : null;
        while ((e = e.next) != null) {//既不是首节点也不是ForwardingNode，那就往下遍历
            if (e.hash == h &&
                ((ek = e.key) == key || (ek != null && key.equals(ek))))
                return e.val;
        }
    }
    return null;
}
```

```java
public V put(K key, V value) {
    return putVal(key, value, false);
}
/** Implementation for put and putIfAbsent */
final V putVal(K key, V value, boolean onlyIfAbsent) {
    if (key == null || value == null) throw new NullPointerException();
    int hash = spread(key.hashCode()); //两次hash，减少hash冲突，可以均匀分布
    int binCount = 0;
    for (Node<K,V>[] tab = table;;) { //对这个table进行迭代
        Node<K,V> f; int n, i, fh;
        //这里就是上面构造方法没有进行初始化，在这里进行判断，为null就调用initTable进行初始化，属于懒汉模式初始化
        if (tab == null || (n = tab.length) == 0)
            tab = initTable();
        else if ((f = tabAt(tab, i = (n - 1) & hash)) == null) {//如果i位置没有数据，就直接无锁插入
            if (casTabAt(tab, i, null,
                         new Node<K,V>(hash, key, value, null)))
                break;                   // no lock when adding to empty bin
        }
        else if ((fh = f.hash) == MOVED)//如果在进行扩容，则先进行扩容操作
            tab = helpTransfer(tab, f);
        else {
            V oldVal = null;
            //如果以上条件都不满足，那就要进行加锁操作，也就是存在hash冲突，锁住链表或者红黑树的头结点
            synchronized (f) {
                if (tabAt(tab, i) == f) {
                    if (fh >= 0) { //表示该节点是链表结构
                        binCount = 1;
                        for (Node<K,V> e = f;; ++binCount) {
                            K ek;
                            //这里涉及到相同的key进行put就会覆盖原先的value
                            if (e.hash == hash &&
                                ((ek = e.key) == key ||
                                 (ek != null && key.equals(ek)))) {
                                oldVal = e.val;
                                if (!onlyIfAbsent)
                                    e.val = value;
                                break;
                            }
                            Node<K,V> pred = e;
                            if ((e = e.next) == null) {  //插入链表尾部
                                pred.next = new Node<K,V>(hash, key,
                                                          value, null);
                                break;
                            }
                        }
                    }
                    else if (f instanceof TreeBin) {//红黑树结构
                        Node<K,V> p;
                        binCount = 2;
                        //红黑树结构旋转插入
                        if ((p = ((TreeBin<K,V>)f).putTreeVal(hash, key,
                                                       value)) != null) {
                            oldVal = p.val;
                            if (!onlyIfAbsent)
                                p.val = value;
                        }
                    }
                }
            }
            if (binCount != 0) { //如果链表的长度大于8时就会进行红黑树的转换
                if (binCount >= TREEIFY_THRESHOLD)
                    treeifyBin(tab, i);
                if (oldVal != null)
                    return oldVal;
                break;
            }
        }
    }
    addCount(1L, binCount);//统计size，并且检查是否需要扩容
    return null;
}
```

```java
private final Node<K,V>[] initTable() {
    Node<K,V>[] tab; int sc;
    while ((tab = table) == null || tab.length == 0) {//空的table才能进入初始化操作
        if ((sc = sizeCtl) < 0) //sizeCtl<0表示其他线程已经在初始化了或者扩容了，挂起当前线程
            Thread.yield(); // lost initialization race; just spin
        else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) {//CAS操作SIZECTL为-1，表示初始化状态
            try {
                if ((tab = table) == null || tab.length == 0) {
                    int n = (sc > 0) ? sc : DEFAULT_CAPACITY;
                    @SuppressWarnings("unchecked")
                    Node<K,V>[] nt = (Node<K,V>[])new Node<?,?>[n];//初始化
                    table = tab = nt;
                    sc = n - (n >>> 2);//记录下次扩容的大小
                }
            } finally {
                sizeCtl = sc;
            }
            break;
        }
    }
    return tab;
}
```

```java
final Node<K,V>[] helpTransfer(Node<K,V>[] tab, Node<K,V> f) {
    Node<K,V>[] nextTab; int sc;
    if (tab != null && (f instanceof ForwardingNode) &&
        (nextTab = ((ForwardingNode<K,V>)f).nextTable) != null) { //新的table nextTba已经存在前提下才能帮助扩容
        int rs = resizeStamp(tab.length);
        while (nextTab == nextTable && table == tab &&
               (sc = sizeCtl) < 0) {
            if ((sc >>> RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 ||
                sc == rs + MAX_RESIZERS || transferIndex <= 0)
                break;
            if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) {
                transfer(tab, nextTab);//调用扩容方法
                break;
            }
        }
        return nextTab;
    }
    return table;
}

private final void transfer(Node<K,V>[] tab, Node<K,V>[] nextTab) {
	int n = tab.length, stride;
	// 每核处理的量小于16，则强制赋值16
	if ((stride = (NCPU > 1) ? (n >>> 3) / NCPU : n) < MIN_TRANSFER_STRIDE)
		stride = MIN_TRANSFER_STRIDE; // subdivide range
	if (nextTab == null) {            // initiating
		try {
			@SuppressWarnings("unchecked")
			Node<K,V>[] nt = (Node<K,V>[])new Node<?,?>[n << 1];        //构建一个nextTable对象，其容量为原来容量的两倍
			nextTab = nt;
		} catch (Throwable ex) {      // try to cope with OOME
			sizeCtl = Integer.MAX_VALUE;
			return;
		}
		nextTable = nextTab;
		transferIndex = n;
	}
	int nextn = nextTab.length;
	// 连接点指针，用于标志位（fwd的hash值为-1，fwd.nextTable=nextTab）
	ForwardingNode<K,V> fwd = new ForwardingNode<K,V>(nextTab);
	// 当advance == true时，表明该节点已经处理过了
	boolean advance = true;
	boolean finishing = false; // to ensure sweep before committing nextTab
	for (int i = 0, bound = 0;;) {
		Node<K,V> f; int fh;
		// 控制 --i ,遍历原hash表中的节点
		while (advance) {
			int nextIndex, nextBound;
			if (--i >= bound || finishing)
				advance = false;
			else if ((nextIndex = transferIndex) <= 0) {
				i = -1;
				advance = false;
			}
			// 用CAS计算得到的transferIndex
			else if (U.compareAndSwapInt
					(this, TRANSFERINDEX, nextIndex,
							nextBound = (nextIndex > stride ?
									nextIndex - stride : 0))) {
				bound = nextBound;
				i = nextIndex - 1;
				advance = false;
			}
		}
		if (i < 0 || i >= n || i + n >= nextn) {
			int sc;
			// 已经完成所有节点复制了
			if (finishing) {
				nextTable = null;
				table = nextTab;        // table 指向nextTable
				sizeCtl = (n << 1) - (n >>> 1);     // sizeCtl阈值为原来的1.5倍
				return;     // 跳出死循环，
			}
			// CAS 更扩容阈值，在这里面sizectl值减一，说明新加入一个线程参与到扩容操作
			if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) {
				if ((sc - 2) != resizeStamp(n) << RESIZE_STAMP_SHIFT)
					return;
				finishing = advance = true;
				i = n; // recheck before commit
			}
		}
		// 遍历的节点为null，则放入到ForwardingNode 指针节点
		else if ((f = tabAt(tab, i)) == null)
			advance = casTabAt(tab, i, null, fwd);
		// f.hash == -1 表示遍历到了ForwardingNode节点，意味着该节点已经处理过了
		// 这里是控制并发扩容的核心
		else if ((fh = f.hash) == MOVED)
			advance = true; // already processed
		else {
			// 节点加锁
			synchronized (f) {
				// 节点复制工作
				if (tabAt(tab, i) == f) {
					Node<K,V> ln, hn;
					// fh >= 0 ,表示为链表节点
					if (fh >= 0) {
						// 构造两个链表  一个是原链表  另一个是原链表的反序排列
						int runBit = fh & n;
						Node<K,V> lastRun = f;
						for (Node<K,V> p = f.next; p != null; p = p.next) {
							int b = p.hash & n;
							if (b != runBit) {
								runBit = b;
								lastRun = p;
							}
						}
						if (runBit == 0) {
							ln = lastRun;
							hn = null;
						}
						else {
							hn = lastRun;
							ln = null;
						}
						for (Node<K,V> p = f; p != lastRun; p = p.next) {
							int ph = p.hash; K pk = p.key; V pv = p.val;
							if ((ph & n) == 0)
								ln = new Node<K,V>(ph, pk, pv, ln);
							else
								hn = new Node<K,V>(ph, pk, pv, hn);
						}
						// 在nextTable i 位置处插上链表
						setTabAt(nextTab, i, ln);
						// 在nextTable i + n 位置处插上链表
						setTabAt(nextTab, i + n, hn);
						// 在table i 位置处插上ForwardingNode 表示该节点已经处理过了
						setTabAt(tab, i, fwd);
						// advance = true 可以执行--i动作，遍历节点
						advance = true;
					}
					// 如果是TreeBin，则按照红黑树进行处理，处理逻辑与上面一致
					else if (f instanceof TreeBin) {
						TreeBin<K,V> t = (TreeBin<K,V>)f;
						TreeNode<K,V> lo = null, loTail = null;
						TreeNode<K,V> hi = null, hiTail = null;
						int lc = 0, hc = 0;
						for (Node<K,V> e = t.first; e != null; e = e.next) {
							int h = e.hash;
							TreeNode<K,V> p = new TreeNode<K,V>
									(h, e.key, e.val, null, null);
							if ((h & n) == 0) {
								if ((p.prev = loTail) == null)
									lo = p;
								else
									loTail.next = p;
								loTail = p;
								++lc;
							}
							else {
								if ((p.prev = hiTail) == null)
									hi = p;
								else
									hiTail.next = p;
								hiTail = p;
								++hc;
							}
						}
						// 扩容后树节点个数若<=6，将树转链表
						ln = (lc <= UNTREEIFY_THRESHOLD) ? untreeify(lo) :
								(hc != 0) ? new TreeBin<K,V>(lo) : t;
						hn = (hc <= UNTREEIFY_THRESHOLD) ? untreeify(hi) :
								(lc != 0) ? new TreeBin<K,V>(hi) : t;
						setTabAt(nextTab, i, ln);
						setTabAt(nextTab, i + n, hn);
						setTabAt(tab, i, fwd);
						advance = true;
					}
				}
			}
		}
	}
}
```

```java
private final void addCount(long x, int check) {
    CounterCell[] as; long b, s;
    //更新baseCount，table的数量，counterCells表示元素个数的变化
    if ((as = counterCells) != null ||
        !U.compareAndSwapLong(this, BASECOUNT, b = baseCount, s = b + x)) {
        CounterCell a; long v; int m;
        boolean uncontended = true;
        //如果多个线程都在执行，则CAS失败，执行fullAddCount，全部加入count
        if (as == null || (m = as.length - 1) < 0 ||
            (a = as[ThreadLocalRandom.getProbe() & m]) == null ||
            !(uncontended =
              U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x))) {
            fullAddCount(x, uncontended);
            return;
        }
        if (check <= 1)
            return;
        s = sumCount();
    }
     //check>=0表示需要进行扩容操作
    if (check >= 0) {
        Node<K,V>[] tab, nt; int n, sc;
        while (s >= (long)(sc = sizeCtl) && (tab = table) != null &&
               (n = tab.length) < MAXIMUM_CAPACITY) {
            int rs = resizeStamp(n);
            if (sc < 0) {
                if ((sc >>> RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 ||
                    sc == rs + MAX_RESIZERS || (nt = nextTable) == null ||
                    transferIndex <= 0)
                    break;
                if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1))
                    transfer(tab, nt);
            }
            //当前线程发起库哦哦让操作，nextTable=null
            else if (U.compareAndSwapInt(this, SIZECTL, sc,
                                         (rs << RESIZE_STAMP_SHIFT) + 2))
                transfer(tab, null);
            s = sumCount();
        }
    }
}
```

## 2. ConcurrentLinkedQueue
<img src="D:\Project\IT-notes\Java\并发\img\ConcurrentLinkedQueue包的继承实现关系.png" style="width:500px;height:400px;" />


```java
private static class Node<E> {
	volatile E item;
	volatile Node<E> next;
	//....
}

public class ConcurrentLinkedQueue<E> extends AbstractQueue<E>
        implements Queue<E>, java.io.Serializable {
        private transient volatile Node<E> head;
        private transient volatile Node<E> tail;  
        //....
}
```

<img src="D:\Project\IT-notes\Java\并发\img\ConcurrentLinkedQueue内部数据结构.png" style="width:700px;height:200px;" />

#### 部分源码
<img src="D:\Project\IT-notes\Java\并发\img\ConcurrentLinkedQueue添加元素过程图.png" style="width:700px;height:400px;" />

```java
public boolean offer(E e) {
	checkNotNull(e);   //为空判断，e为null是抛异常
	final Node<E> newNode = new Node<E>(e); //将e包装成newNode
	for (Node<E> t = tail, p = t;;) {  //循环cas，直至加入成功
		//t = p = tail 
		Node<E> q = p.next;
		if (q == null) {   //判断p是否为尾节点
			//如果是，p.next = newNode
			if (p.casNext(null, newNode)) {
				//首次添加时，p 等于t，不进行尾节点更新，所以所尾节点存在滞后性  
				//并发环境，可能存添加/删除，tail就更难保证正确指向最后节点。
				if (p != t) 
					//更新尾节点为最新元素
					casTail(t, newNode);  
				return true;
			}
		}
		else if (p == q)
			//当tail不执行最后节点时，如果执行出列操作，很有可能将tail也给移除了    
			//此时需要对tail节点进行复位，复位到head节点
			p = (t != (t = tail)) ? t : head;
		else
			//推动tail尾节点往队尾移动
			p = (p != t && t != (t = tail)) ? t : q;
	}
}
```

<img src="D:\Project\IT-notes\Java\并发\img\ConcurrentLinkedQueue去除元素过程图.png" style="width:700px;height:400px;" />

```java
public E poll() {
	restartFromHead:
	for (;;) {
		for (Node<E> h = head, p = h, q;;) {
			//入列折腾的tail，那出列折腾的就是head
			E item = p.item;
			//出列判断依据是节点的item=null
			//item ！= null， 并且能将操作节点的item设置null， 表示出列成功
			if (item != null && p.casItem(item, null)) {
				if (p != h) 
					//一旦出列成功需要对head进行移动
					updateHead(h, ((q = p.next) != null) ? q : p);
				return item;
			}
			else if ((q = p.next) == null) {
				updateHead(h, p);
				return null;
			}
			else if (p == q)
				//第一轮操作失败，下一轮继续，调回到循环前
				continue restartFromHead;
			else
				//推动head节点移动
				p = q;
		}
	}
}
```

## 3. 阻塞队列
`ConcurrentLinkedQueue`使用的是CAS+自旋的非阻塞方式实现多线程访问，而Java中还存在多种阻塞队列：
- `ArrayBlockingQueue`：一个由数组结构组成的有界阻塞队列
- `LinkedBlockingQueue`：一个由链表结构组成的有界阻塞队列
- `PriorityBlockingQueue`：一个支持优先级排序的无界阻塞队列
- `DelayQueue`：一个使用优先级队列实现的无界阻塞队列
- `SynchronousQueue`：一个不存储元素的阻塞队列
- `LinkedTransferQueue`：一个由链表结构组成的无界阻塞队列
- `LinkedBlockingDeque`：一个由链表结构组成的双向阻塞队列

#### 阻塞队列的实现原理，以ArrayBlockingQueue为例：
```java
//其实就是一个消费者/生产者模型
private final Condition notFull;
private final Condition notEmpty;
public ArrayBlockingQueue(int capacity, boolean fair)
{
    // 省略其他代码
    notEmpty = lock.newCondition();
    notFull = lock.newCondition();
}
public void put(E e) throws InterruptedException
{
    checkNotNull(e);
    final ReentrantLock lock = this.lock;
    lock.lockInterruptibly();
    try
    {
        while(count == items.length) notFull.await();
        insert(e);
    }
    finally
    {
        lock.unlock();
    }
}
public E take() throws InterruptedException
{
    final ReentrantLock lock = this.lock;
    lock.lockInterruptibly();
    try
    {
        while(count == 0) notEmpty.await();
        return extract();
    }
    finally
    {
        lock.unlock();
    }
}
private void insert(E x)
{
    items[putIndex] = x;
    putIndex = inc(putIndex);
    ++count;
    notEmpty.signal();
}
```

## 4. Fork/Join框架
工作窃取（`work-stealing`）算法是指某个线程从其他队列里窃取任务来执行。假如我们需要做一个比较大的任务，可以把这个任务分割为若干互不依赖的子任务，为了减少线程间的竞争，把这些子任务分别放到不同的队列里，并为每个队列创建一个单独的线程来执行队列里的任务，线程和队列一一对应

有的线程会先把自己队列里的任务干完，而其他线程对应的队列里还有任务等待处理。干完活的线程与其等着，不如去帮其他线程干活，于是它就去其他线程的队列里窃取一个任务来执行。而在这时它们会访问同一个队列，所以为了减少窃取任务线程和被窃取任务线程之间的竞争，通常会使用双端队列，被窃取任务线程永远从双端队列的头部拿任务执行，而窃取任务的线程永远从双端队列的尾部拿任务执行

<img src="D:\Project\IT-notes\Java\并发\img\工作窃取.png" style="width:400px;height:400px;" />

#### fork/join示例
```java
package fj;
import java.util.concurrent.ExecutionException;
import java.util.concurrent.ForkJoinPool;
import java.util.concurrent.Future;
import java.util.concurrent.RecursiveTask;
public class CountTask extends RecursiveTask < Integer >
{
    private static final int THRESHOLD = 2; // 阈值
    private int start;
    private int end;
    public CountTask(int start, int end)
    {
        this.start = start;
        this.end = end;
    }
    @Override
    protected Integer compute()
    {
        int sum = 0;
        // 如果任务足够小就计算任务
        boolean canCompute = (end - start) <= THRESHOLD;
        if(canCompute)
        {
            for(int i = start; i <= end; i++)
            {
                sum += i;
            }
        }
        else
        {
            // 如果任务大于阈值，就分裂成两个子任务计算
            int middle = (start + end) / 2;
            CountTask leftTask = new CountTask(start, middle);
            CountTask rightTask = new CountTask(middle + 1, end);
            // 执行子任务
            leftTask.fork();
            rightTask.fork();
            // 等待子任务执行完，并得到其结果
            int leftResult = leftTask.join();
            int rightResult = rightTask.join();
            // 合并子任务
            sum = leftResult + rightResult;
        }
        return sum;
    }
    public static void main(String[] args)
    {
        ForkJoinPool forkJoinPool = new ForkJoinPool();
        // 生成一个计算任务，负责计算1+2+3+4
        CountTask task = new CountTask(1, 4);
        // 执行一个任务
        Future < Integer > result = forkJoinPool.submit(task);
        try
        {
            System.out.println(result.get());
        }
        catch(InterruptedException e)
        {}
        catch(ExecutionException e)
        {}
    }
}
```

### 1. fork/join框架的部分实现原理
`ForkJoinPool`由`ForkJoinTask`数组和`ForkJoinWorkerThread`数组组成，`ForkJoinTask`数组负责将存放程序提交给`ForkJoinPool`的任务，而`ForkJoinWorkerThread`数组负责执行这些任务
```java
//fork的实现原理
public final ForkJoinTask < V > fork()
{
    ((ForkJoinWorkerThread) Thread.currentThread()).pushTask(this);
    return this;
}

final void pushTask(ForkJoinTask < > t)
{
    ForkJoinTask < > [] q;
    int s, m;
    if((q = queue) != null)
    { // ignore if queue removed
        long u = (((s = queueTop) & (m = q.length - 1)) << ASHIFT) + ABASE;
        UNSAFE.putOrderedObject(q, u, t);
        queueTop = s + 1; // or use putOrderedInt
        if((s -= queueBase) <= 2) pool.signalWork();
        else if(s == m) growQueue();
    }
}
```

```java
//join的实现原理
public final V join()
{
    if(doJoin() != NORMAL) return reportResult();
    else return getRawResult();
}
private V reportResult()
{
    int s;
    Throwable ex;
    if((s = status) == CANCELLED) throw new CancellationException();
    if(s == EXCEPTIONAL && (ex = getThrowableException()) != null) UNSAFE.throwException(ex);
    return getRawResult();
}
```